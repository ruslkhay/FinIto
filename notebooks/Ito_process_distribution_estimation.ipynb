{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# About\n",
    "Estimation of distribution of dynamical and diffusional parameters of Ito's \n",
    "processes. This approach don't imply any functional representation of parameters\n",
    "as in most of the know models.\n",
    "\n",
    "## Brief overview\n",
    "\n",
    "$S(t) = S_t$ is called an Ito process if it satisfies next equality:\n",
    "$$ \n",
    "dS_t = a(t, S_t) \\cdot dt + b(t, S_t) \\cdot dW \n",
    "$$ (Ito_proc)\n",
    "\n",
    "\n",
    "We can create estimate $a(t, S_t)$ and $b(t, S_t)$ in two ways:\n",
    "1. By their distribution\n",
    "2. Point-to-point\n",
    "\n",
    "\n",
    "## 1. Estimation of Process distribution\n",
    "\n",
    "Let $S(t_i) = S_i, \\quad a(t_i, S_i) = a_i, \\quad b(t_i, S_i) = b_i$ and\n",
    "$ i \\in \\{1, ..., T\\}$. Then\n",
    "$$\n",
    "\\Delta S = S_i - S_{i-1} \\approx a(t_i, S_i) \\Delta t + b(t_i, S_i) \\Delta W \n",
    "$$ (my_label)\n",
    "\n",
    "$$\n",
    "\\Rightarrow\n",
    "\\mathbb{P}(\\Delta S < x) \\approx \\mathbb{P}(\\Delta W < \\frac{x - a_i \\Delta t}{b_i}),\n",
    "\\quad \\text{where } \\Delta W \\sim \\mathcal{N}(0,\\Delta t)\\,\n",
    "$$\n",
    "\n",
    "Here $\\Delta t$ denotes time discretization step.\n",
    "\n",
    "### Law of total probability\n",
    "\n",
    "Let denote $\\frac{x - a_i \\Delta t}{b_i} = \\xi(x, a_i, b_i, t)= \\xi$. It is a r.v. \n",
    "Therefore $\\mathbb{P}(\\Delta W < \\xi) \\equiv \\Phi(\\frac{\\xi - 0}{\\Delta t})$\n",
    "\n",
    "Keeping in mind that $a_i$ and $b_i$ are r.v. Without loss of generality \n",
    "let's suppose that $\\forall t: \\; a_i \\in A \\subset \\mathbb{R} \\wedge b_i \\in B \\subset \\mathbb{R^+j}$.\n",
    "\n",
    "Let $f_{ab}$ denote joint distribution density for r.v. $a_i$ and $b_i$.\n",
    "Then\n",
    "$$\n",
    "\\mathbb{P}(\\Delta W < \\xi) = \n",
    "\\int_{A}\\int_{B} \\, \\mathbb{P}(\\Delta W < \\xi \\: | \\:a_i=y \\wedge b_i=z) \\cdot f_{ab}(y, z)\\, dy \\, dz =\n",
    "\\newline\n",
    "\\int_{A}\\int_{B} \\, \\Phi(\\frac{x - y \\Delta t}{z \\Delta t}) \\cdot f_{ab}(y, z)\\, dy \\, dz =\n",
    "\\{\\;f_{ab}(y_k, z_k) = p_k \\;\\} \\approx\n",
    "\\sum_{k=1}^{K}{p_k \\cdot \\Phi \\left( \\frac{x - y_k \\Delta t}{z_k \\Delta t} \\right)} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Small note: as a result we can get that} \\quad\n",
    "\\mathbb{P}(dW < \\xi) = \n",
    "\\mathbb{E}_{a,b}(\\Phi(\\frac{x - a \\cdot dt}{b \\cdot dt})) \n",
    "$$\n",
    "\n",
    "Therefore we can approximate initial process distribution as:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\Delta S < x) \\approx\n",
    "\\sum_{k=1}^{K}{p_k \\cdot \\Phi \\left( \\frac{x - y_k \\Delta t}{z_k \\Delta t} \\right)} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Aspects to consider\n",
    "1. How to choose limits for $A$ and $B$? Is it bad to consider them constant for\n",
    "each $t$?\n",
    "2. How to choose points $\\{(y_k, z_k)\\}_{k=1}^{K}$? Obviously the best approach \n",
    "is to select extremum of the joint distribution function $f_{ab}$.\n",
    "3. Are $a$ and $b$ dependant r.v.? $\\Leftrightarrow$ Can we assume $f_{ab} = f_a \\cdot f_b$?\n",
    "\n",
    "Answer to question 3 define dimensionality of optimization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finito.simulator import generateGeneralWiener\n",
    "from finito.gauss import GaussianMixture\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate process\n",
    "dummyS = generateGeneralWiener(\n",
    "    a=3, b=0.4, dt=np.timedelta64(1, \"ms\"), T=np.timedelta64(10, \"s\")\n",
    ")\n",
    "print(\"Shape of data:\", dummyS.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### 1.1. EM-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take differences\n",
    "deltaS = np.diff(dummyS)\n",
    "\n",
    "# Take a look on result\n",
    "f, ax = plt.subplots(3, 1, figsize=(10, 10))\n",
    "ax[0].plot(dummyS)\n",
    "ax[0].set_title(\"Original process\")\n",
    "ax[1].plot(deltaS)\n",
    "ax[1].set_title(\"Differences\")\n",
    "sns.histplot(deltaS, kde=True, ax=ax[2])\n",
    "ax[2].set_title(\"Distribution of differences\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as GM\n",
    "\n",
    "gmm = GM(\n",
    "    n_components=1,\n",
    ")\n",
    "\n",
    "gmm.fit(deltaS.reshape(-1, 1))\n",
    "\n",
    "print(\n",
    "    \"Model parameters after fitting:\",\n",
    "    f\"Means: {gmm.means_[0]}\",\n",
    "    f\"Covariances: {gmm.covariances_.flatten()}\",\n",
    "    sep=\"\\n\",\n",
    ")\n",
    "__time_delta = np.timedelta64(1, \"s\") / np.timedelta64(1, \"ms\")\n",
    "print(\n",
    "    \"As we can see it differs from initial a and b parameters on time delta:\",\n",
    "    __time_delta,\n",
    ")\n",
    "print(\n",
    "    f\"Means: {gmm.means_ * __time_delta}\",\n",
    "    f\"Covariances: {gmm.covariances_ * __time_delta}\",\n",
    "    f\"Standard deviation: {np.sqrt(gmm.covariances_ * __time_delta)}\",\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 1.2. Minimization problem on distribution\n",
    "\n",
    "Discrete approximation $F_{\\Delta S, T}(x)$ of the continuous distribution \n",
    "$F_{dS}(x)$ can be estimated in two ways: theoretical and empirical. \n",
    "\n",
    "On one hand:\n",
    "$$\n",
    "F^{(theor)}_{\\Delta S, T}(x) = \\mathbb{P}(\\Delta S < x) \\approx \n",
    "\\sum_{k=1}^{K}{p_k \\cdot \\Phi \\left( \\frac{x - y_k \\Delta t}{z_k \\Delta t} \\right)} \n",
    "\\xrightarrow{K \\rightarrow \\infty , \\,\\Delta t \\rightarrow 0} F_{dS}\n",
    "$$\n",
    "\n",
    "But on the other we can estimate empirical distribution of $\\Delta S$ as:\n",
    "\n",
    "$$\n",
    "F^{(emp)}_{\\Delta S, T}(x) = \n",
    "\\frac{1}{T} \\cdot \\sum_{j=1}^{T} \\mathbb{I} \\left(\\Delta S < x\\right)\n",
    "\\xrightarrow{\\Delta t \\rightarrow 0} F_{dS}\n",
    "$$\n",
    "\n",
    "For evaluation of parameters $V_k = \\{y_k, z_k, p_k\\}$ we can set an \n",
    "minimization problem on distance (in some metric $\\rho(f,g)$) between\n",
    "theoretical and empirical distributions:\n",
    "\n",
    "$$\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\begin{equation}\n",
    "\\left\\{ \\begin{aligned} \n",
    "\n",
    "    \\min_{\\{V_k\\}_{k=1}^{K}} \\rho \\left(F^{(emp)}_{\\Delta S, T}, F^{(theor)}_{\\Delta S, T}\\right) \\\\\n",
    "    p_k \\geq 0 \\; \\wedge \\; \\sum_{k=1}^{K}p_k = 1 \\\\\n",
    "    0 \\leq z_k \\in B \\subset \\mathbb{R^+} \\\\\n",
    "    y_k \\in A \\subset \\mathbb{R}\n",
    "\n",
    "\n",
    "\n",
    "\\end{aligned} \\right.\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### 1.2.1. Empirical distribution implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optimize cycle. Currently O(M*N), can be O(N), where M is len(x), N is len(data)\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def EmpiricalCDF(data: Iterable, x: Iterable | None = None):\n",
    "    \"\"\"Estimate cumulative distribution function.\n",
    "\n",
    "    Args:\n",
    "        data (Iterable): Samples of the random variable\n",
    "        x (Iterable | None): Specific points to calculate distribution in\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Use all possible information for estimation\n",
    "    if x is None:\n",
    "        x = data\n",
    "    res = []\n",
    "    nSamples = len(data)\n",
    "    for xi in x:\n",
    "        res.append(np.sum(data <= xi) / nSamples)\n",
    "    res.sort()\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent empirical distribution function correspondence to theoretical\n",
    "# distributions for a basic distributions\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Binomial\n",
    "ax[0].set_title(\"Binomial distribution\")\n",
    "ax[1].set_title(\"Normal distribution\")\n",
    "binomRV = stats.binom(n=10, p=0.5)\n",
    "X = np.arange(-3, 13)\n",
    "sns.pointplot(x=X, y=binomRV.cdf(X), ax=ax[0], color=\"blue\")\n",
    "nSamples = 240\n",
    "binomSamples = binomRV.rvs(size=nSamples)\n",
    "X = np.arange(min(binomSamples), max(binomSamples))\n",
    "Xm = X[::1]  # To select discretization step\n",
    "sns.pointplot(x=Xm, y=EmpiricalCDF(binomSamples, Xm), ax=ax[0], color=\"orange\")\n",
    "# Standard normal\n",
    "nSamples = 200\n",
    "normRV = stats.norm(loc=0, scale=1)\n",
    "X = np.linspace(-3, 3, nSamples)\n",
    "sns.pointplot(x=X, y=normRV.cdf(X), ax=ax[1], color=\"blue\")\n",
    "normSamples = normRV.rvs(size=nSamples)\n",
    "Xm = X[::10]\n",
    "sns.pointplot(x=Xm, y=EmpiricalCDF(normSamples, Xm), ax=ax[1], color=\"orange\")\n",
    "ax[1].tick_params(axis=\"x\", which=\"both\", labelsize=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### 1.2.2 Gaussian mixture model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [-1, 3, 5]\n",
    "b = [0.3, 0.5, 0.2]\n",
    "\n",
    "gm = GaussianMixture(a=a, b=b)\n",
    "r = gm.get_params()\n",
    "np.stack(r)\n",
    "gm.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [-1, 3, 5]\n",
    "b = [0.3, 0.5, 0.2]\n",
    "\n",
    "gm1 = GaussianMixture(a=a, b=b)\n",
    "x_grid = np.linspace(-3, 7, 100)\n",
    "cdf_values = gm1.cdf(x_grid)\n",
    "print(\n",
    "    \"Plots are different time to time due to random weights generation.\",\n",
    "    f\"\\nCurrent weights: {gm1._weights.flatten()}\",\n",
    ")\n",
    "\n",
    "pdf_values = gm1.pdf(x_grid)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_grid, pdf_values, label=\"GMM PDF\", color=\"blue\")\n",
    "plt.title(\"PDF of Gaussian Mixture Model\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(gm1.sample(size=1000), kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "samples = gm1.sample(500)\n",
    "x = sorted(samples)\n",
    "plt.figure(figsize=(8, 5))\n",
    "origCDF = gm1.cdf(x)\n",
    "plt.plot(x, origCDF, label=\"Original CDF\", color=\"black\")\n",
    "empirCDF = EmpiricalCDF(samples)\n",
    "plt.plot(x, empirCDF, label=\"Empirical CDF\", color=\"green\")\n",
    "plt.title(\"CDF of Gaussian Mixture Model\")\n",
    "plt.xlabel(\"Random variable values\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Number of samples: {len(samples)}\",\n",
    "    f\"Mean absolute error: {mean_absolute_error(origCDF, empirCDF) * 100:.2f}%\",\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import LinearConstraint, OptimizeResult\n",
    "\n",
    "\n",
    "def distance(params, x, dist=np.abs):\n",
    "    mu, sigma, weight = params.reshape(3, 3, -1)\n",
    "    F_n = EmpiricalCDF(x)\n",
    "    gaus = stats.norm(loc=mu, scale=sigma)\n",
    "    F_theory = np.sum(weight * gaus.cdf(sorted(x)), axis=0)\n",
    "    # Evaluate KS statistic: max |F_n(t) - F(t; mu, sigma)|\n",
    "    ks_vals = dist(F_n - F_theory)\n",
    "    return float(np.max(ks_vals))\n",
    "\n",
    "\n",
    "def distF(x):\n",
    "    return lambda params: distance(params, x)\n",
    "\n",
    "\n",
    "bounds = [\n",
    "    (-5, 5),\n",
    "    (-5, 5),\n",
    "    (-5, 5),\n",
    "    (1e-2, 1),\n",
    "    (1e-2, 1),\n",
    "    (1e-2, 1),\n",
    "    (0, 1),\n",
    "    (0, 1),\n",
    "    (0, 1),\n",
    "]\n",
    "initial_guess = [\n",
    "    np.random.uniform(*bounds[0], 3),\n",
    "    np.random.uniform(*bounds[3], 3),\n",
    "    np.random.dirichlet(np.ones(3)),\n",
    "]\n",
    "initial_guess = np.array(initial_guess).flatten()\n",
    "A = np.array(\n",
    "    [\n",
    "        [0, 0, 0, 0, 0, 0, 1, 1, 1]  # Constraint on probs sum\n",
    "    ]\n",
    ")\n",
    "lb = np.array([1])  # Lower bounds for the constraints\n",
    "ub = np.array([1])  # Lower bounds for the constraints\n",
    "linear_constraint = LinearConstraint(A, lb, ub)\n",
    "\n",
    "\n",
    "# res = dual_annealing(\n",
    "# res = basinhopping(\n",
    "\n",
    "res = OptimizeResult(fun=float(\"inf\"))\n",
    "while res.fun > 0.02:\n",
    "    res = minimize(\n",
    "        distF(samples),\n",
    "        initial_guess,\n",
    "        bounds=bounds,\n",
    "        constraints=[linear_constraint],\n",
    "        method=\"SLSQP\",\n",
    "    )\n",
    "    initial_guess = [\n",
    "        np.random.uniform(*bounds[0], 3),\n",
    "        np.random.uniform(*bounds[3], 3),\n",
    "        np.random.dirichlet(np.ones(3)),\n",
    "    ]\n",
    "    initial_guess = np.array(initial_guess).flatten()\n",
    "    print(res.fun)\n",
    "print(res)\n",
    "predictions = res.x.reshape(3, -1)\n",
    "predictions = [list(map(lambda x: float(round(x, 6)), param)) for param in predictions]\n",
    "print(\n",
    "    \"Predicted parameters\",\n",
    "    *predictions,\n",
    "    \"Original parameters\",\n",
    "    *np.stack(gm1.get_params()),\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmP = GaussianMixture(*predictions)\n",
    "x = sorted(samples)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8 * 2, 5))\n",
    "origCDF = gm1.cdf(x)\n",
    "axes[0].plot(x, origCDF, label=\"Original CDF\", color=\"black\")\n",
    "empirCDF = EmpiricalCDF(samples)\n",
    "axes[0].plot(x, empirCDF, label=\"Empirical CDF\", color=\"green\")\n",
    "axes[0].plot(x, gmP.cdf(x), label=\"Predicted GMM CDF\", color=\"red\")\n",
    "axes[0].set_title(\"CDF of Gaussian Mixture Model\")\n",
    "axes[0].set_xlabel(\"Random variable values\")\n",
    "axes[0].set_ylabel(\"Probability\")\n",
    "axes[0].legend()\n",
    "axes[0].grid()\n",
    "\n",
    "axes[1].plot(x, gm1.pdf(x), label=\"Original PDF\", color=\"black\")\n",
    "empirCDF = EmpiricalCDF(samples)\n",
    "# axes[1].plot(x, numDeriv(empirCDF, x), label=\"Empirical PDF\", color=\"green\")\n",
    "axes[1].plot(x, gmP.pdf(x), label=\"Predicted GMM PDF\", color=\"red\")\n",
    "axes[1].set_title(\"PDF of Gaussian Mixture Model\")\n",
    "axes[1].set_xlabel(\"Random variable values\")\n",
    "axes[1].set_ylabel(\"Probability\")\n",
    "axes[1].legend()\n",
    "axes[1].grid()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
