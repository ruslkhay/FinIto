{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# About\n",
    "Estimation of distribution of dynamical and diffusional parameters of Ito's \n",
    "processes. This approach don't imply any functional representation of parameters\n",
    "as in most of the know models.\n",
    "\n",
    "## Brief overview\n",
    "\n",
    "$S(t) = S_t$ is called an Ito process if it satisfies next equality:\n",
    "$$ \n",
    "dS_t = a(t, S_t) \\cdot dt + b(t, S_t) \\cdot dW \n",
    "$$ (Ito_proc)\n",
    "\n",
    "\n",
    "We can create estimate $a(t, S_t)$ and $b(t, S_t)$ in two ways:\n",
    "1. By their distribution\n",
    "2. Point-to-point\n",
    "\n",
    "\n",
    "## 1. Estimation of Process distribution\n",
    "\n",
    "Let $S(t_i) = S_i, \\quad a(t_i, S_i) = a_i, \\quad b(t_i, S_i) = b_i$ and\n",
    "$ i \\in \\{1, ..., T\\}$. Then\n",
    "$$\n",
    "\\Delta S = S_i - S_{i-1} \\approx a(t_i, S_i) \\Delta t + b(t_i, S_i) \\Delta W \n",
    "$$ (my_label)\n",
    "\n",
    "$$\n",
    "\\Rightarrow\n",
    "\\mathbb{P}(\\Delta S < x) \\approx \\mathbb{P}(\\Delta W < \\frac{x - a_i \\Delta t}{b_i}),\n",
    "\\quad \\text{where } \\Delta W \\sim \\mathcal{N}(0,\\Delta t)\\,\n",
    "$$\n",
    "\n",
    "Here $\\Delta t$ denotes time discretization step.\n",
    "\n",
    "### Law of total probability\n",
    "\n",
    "Let denote $\\frac{x - a_i \\Delta t}{b_i} = \\xi(x, a_i, b_i, t)= \\xi$. It is a r.v. \n",
    "Therefore $\\mathbb{P}(\\Delta W < \\xi) \\equiv \\Phi(\\frac{\\xi - 0}{\\Delta t})$\n",
    "\n",
    "Keeping in mind that $a_i$ and $b_i$ are r.v. Without loss of generality \n",
    "let's suppose that $\\forall t: \\; a_i \\in A \\subset \\mathbb{R} \\wedge b_i \\in B \\subset \\mathbb{R}$.\n",
    "\n",
    "Let $f_{ab}$ denote joint distribution density for r.v. $a_i$ and $b_i$.\n",
    "Then\n",
    "$$\n",
    "\\mathbb{P}(\\Delta W < \\xi) = \n",
    "\\int_{A}\\int_{B} \\, \\mathbb{P}(\\Delta W < \\xi \\: | \\:a_i=y \\wedge b_i=z) \\cdot f_{ab}(y, z)\\, dy \\, dz =\n",
    "\\newline\n",
    "\\int_{A}\\int_{B} \\, \\Phi(\\frac{x - y \\Delta t}{z \\Delta t}) \\cdot f_{ab}(y, z)\\, dy \\, dz =\n",
    "\\{\\;f_{ab}(y_k, z_k) = p_k \\;\\} \\approx\n",
    "\\sum_{k=1}^{K}{p_k \\cdot \\Phi \\left( \\frac{x - y_k \\Delta t}{z_k \\Delta t} \\right)} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Small note: as a result we can get that} \\quad\n",
    "\\mathbb{P}(dW < \\xi) = \n",
    "\\mathbb{E}_{a,b}(\\Phi(\\frac{x - a \\cdot dt}{b \\cdot dt})) \n",
    "$$\n",
    "\n",
    "Therefore we can approximate initial process distribution as:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\Delta S < x) \\approx\n",
    "\\sum_{k=1}^{K}{p_k \\cdot \\Phi \\left( \\frac{x - y_k \\Delta t}{z_k \\Delta t} \\right)} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Aspects to consider\n",
    "1. How to choose limits for $A$ and $B$? Is it bad to consider them constant for\n",
    "each $t$?\n",
    "2. How to choose points $\\{(y_k, z_k)\\}_{k=1}^{K}$? Obviously the best approach \n",
    "is to select extremum of the joint distribution function $f_{ab}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finito.simulator import generateGeneralWiener\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate process\n",
    "dummyS = generateGeneralWiener(\n",
    "    a=3, b=0.4, dt=np.timedelta64(1, \"ms\"), T=np.timedelta64(10, \"s\")\n",
    ")\n",
    "print(\"Shape of data:\", dummyS.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### 1.1. EM-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take differences\n",
    "deltaS = np.diff(dummyS)\n",
    "\n",
    "# Take a look on result\n",
    "f, ax = plt.subplots(3, 1, figsize=(10, 10))\n",
    "ax[0].plot(dummyS)\n",
    "ax[0].set_title(\"Original process\")\n",
    "ax[1].plot(deltaS)\n",
    "ax[1].set_title(\"Differences\")\n",
    "sns.histplot(deltaS, kde=True, ax=ax[2])\n",
    "ax[2].set_title(\"Distribution of differences\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(\n",
    "    n_components=1,\n",
    ")\n",
    "\n",
    "gmm.fit(deltaS.reshape(-1, 1))\n",
    "\n",
    "print(\n",
    "    \"Model parameters after fitting:\",\n",
    "    f\"Means: {gmm.means_[0]}\",\n",
    "    f\"Covariances: {gmm.covariances_.flatten()}\",\n",
    "    sep=\"\\n\",\n",
    ")\n",
    "__time_delta = np.timedelta64(1, \"s\") / np.timedelta64(1, \"ms\")\n",
    "print(\n",
    "    \"As we can see it differs from initial a and b parameters on time delta:\",\n",
    "    __time_delta,\n",
    ")\n",
    "print(\n",
    "    f\"Means: {gmm.means_ * __time_delta}\",\n",
    "    f\"Covariances: {gmm.covariances_ * __time_delta}\",\n",
    "    f\"Standard deviation: {np.sqrt(gmm.covariances_ * __time_delta)}\",\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 1.2. Minimization problem on distribution\n",
    "\n",
    "Discrete approximation $F_{\\Delta S, T}(x)$ of the continuous distribution \n",
    "$F_{dS}(x)$ can be estimated in two ways: theoretical and empirical. \n",
    "\n",
    "On one hand:\n",
    "$$\n",
    "F^{(theor)}_{\\Delta S, T}(x) = \\mathbb{P}(\\Delta S < x) \\approx \n",
    "\\sum_{k=1}^{K}{p_k \\cdot \\Phi \\left( \\frac{x - y_k \\Delta t}{z_k \\Delta t} \\right)} \n",
    "\\xrightarrow{K \\rightarrow \\infty , \\,\\Delta t \\rightarrow 0} F_{dS}\n",
    "$$\n",
    "\n",
    "But on the other we can estimate empirical distribution of $\\Delta S$ as:\n",
    "\n",
    "$$\n",
    "F^{(emp)}_{\\Delta S, T}(x) = \n",
    "\\frac{1}{T} \\cdot \\sum_{j=1}^{T} \\mathbb{I} \\left(\\Delta S < x\\right)\n",
    "\\xrightarrow{\\Delta t \\rightarrow 0} F_{dS}\n",
    "$$\n",
    "\n",
    "For evaluation of parameters $V_k = \\{a_k, b_k, p_k\\}$ we can set an \n",
    "minimization problem on distance (in some metric $\\rho(f,g)$) between\n",
    "theoretical and empirical distributions:\n",
    "\n",
    "$$\n",
    "\\min_{\\{V_k\\}_{k=1}^{K}} \\rho \\left(F^{(emp)}_{\\Delta S, T}, F^{(theor)}_{\\Delta S, T}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### 1.2.1. Empirical distribution implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optimize cycle. Currently O(M*N), can be O(N), where M is len(x), N is len(data)\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def EmpiricalCDF(data: Iterable, x: Iterable | None = None):\n",
    "    \"\"\"Estimate cumulative distribution function.\n",
    "\n",
    "    Args:\n",
    "        data (Iterable): Samples of the random variable\n",
    "        x (Iterable | None): Specific points to calculate distribution in\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Use all possible information for estimation\n",
    "    if x is None:\n",
    "        x = data\n",
    "    res = []\n",
    "    nSamples = len(data)\n",
    "    for xi in x:\n",
    "        res.append(np.sum(data <= xi) / nSamples)\n",
    "    res.sort()\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent empirical distribution function correspondence to theoretical\n",
    "# distributions for a basic distributions\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Binomial\n",
    "ax[0].set_title(\"Binomial distribution\")\n",
    "ax[1].set_title(\"Normal distribution\")\n",
    "binomRV = stats.binom(n=10, p=0.5)\n",
    "X = np.arange(-3, 13)\n",
    "sns.pointplot(x=X, y=binomRV.cdf(X), ax=ax[0], color=\"blue\")\n",
    "nSamples = 240\n",
    "binomSamples = binomRV.rvs(size=nSamples)\n",
    "X = np.arange(min(binomSamples), max(binomSamples))\n",
    "Xm = X[::1]  # To select discretization step\n",
    "sns.pointplot(x=Xm, y=EmpiricalCDF(binomSamples, Xm), ax=ax[0], color=\"orange\")\n",
    "# Standard normal\n",
    "nSamples = 200\n",
    "normRV = stats.norm(loc=0, scale=1)\n",
    "X = np.linspace(-3, 3, nSamples)\n",
    "sns.pointplot(x=X, y=normRV.cdf(X), ax=ax[1], color=\"blue\")\n",
    "normSamples = normRV.rvs(size=nSamples)\n",
    "Xm = X[::10]\n",
    "sns.pointplot(x=Xm, y=EmpiricalCDF(normSamples, Xm), ax=ax[1], color=\"orange\")\n",
    "ax[1].tick_params(axis=\"x\", which=\"both\", labelsize=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### 1.2.2 Gaussian mixture model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixture:\n",
    "    def _CheckGMParameters(a, b, p) -> None:\n",
    "        assert a is not None and b is not None, \"Means and variances must be provided\"\n",
    "        assert a.shape == b.shape == p.shape, \"Parameters must have the same length\"\n",
    "        # Variances\n",
    "        assert np.all(b > 0), \"Variances must be positive\"\n",
    "        # Probabilities\n",
    "        assert np.isclose(np.sum(p), 1.0), \"Weights must sum to 1\"\n",
    "        assert np.all(p >= 0), \"Weights must be non-negative\"\n",
    "\n",
    "    def __init__(self, a, b, p=None):\n",
    "        \"\"\"Initialize a Gaussian Mixture Model (GMM) with given parameters.\n",
    "\n",
    "        Parameters:\n",
    "            a (list[float]): means of the components\n",
    "            b (list[float]): variances of the components\n",
    "            p (list[float] | None): weights of the components (should sum to 1).\n",
    "                If None, random weights will be generated.\n",
    "        \"\"\"\n",
    "        a = np.asarray(a).reshape(-1, 1)  # (K, 1)\n",
    "        b = np.asarray(b).reshape(-1, 1)  # (K, 1)\n",
    "        if p is None:\n",
    "            # Generate random weights that sum to 1\n",
    "            p = np.random.dirichlet(np.ones(b.shape[0]))\n",
    "        p = np.asarray(p).reshape(-1, 1)  # (K, 1)\n",
    "        __class__._CheckGMParameters(a, b, p)\n",
    "        self._means = a\n",
    "        self._variances = b\n",
    "        self._weights = p\n",
    "        self.n_components = a.shape[0]\n",
    "        self._gaussians = stats.norm(loc=self._means, scale=np.sqrt(self._variances))\n",
    "\n",
    "    def cdf(self, x):\n",
    "        \"\"\"Compute the cumulative distribution function for the GMM at points x.\"\"\"\n",
    "        return np.sum(self._weights * self._gaussians.cdf(x), axis=0)\n",
    "\n",
    "    def pdf(self, x):\n",
    "        \"\"\"Compute the probability density function for the GMM at points x.\"\"\"\n",
    "        return np.sum(self._weights * self._gaussians.pdf(x), axis=0)\n",
    "\n",
    "    def sample(self, size, random_state=None):\n",
    "        \"\"\"Generate random samples from the GMM.\"\"\"\n",
    "        # Choose what gaussian to use for generation based on components weights\n",
    "        component_indices = np.random.choice(\n",
    "            self.n_components, size=size, p=self._weights.flatten()\n",
    "        )\n",
    "        # Generate samples from the chosen components\n",
    "        samples = np.array(\n",
    "            [\n",
    "                self._gaussians.rvs(\n",
    "                    size=(self.n_components, 1), random_state=random_state\n",
    "                )[idx][0]\n",
    "                for idx in component_indices\n",
    "            ]\n",
    "        )\n",
    "        return samples\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"Return the parameters of the GMM as a tuple (means, variances, weights).\"\"\"\n",
    "        return self._means.flatten(), self._variances.flatten(), self._weights.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [-1, 3, 5]\n",
    "b = [0.3, 0.5, 0.2]\n",
    "\n",
    "gm = GaussianMixture(a=a, b=b)\n",
    "r = gm.get_params()\n",
    "np.stack(r)\n",
    "gm.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [-1, 3, 5]\n",
    "b = [0.3, 0.5, 0.2]\n",
    "\n",
    "gm1 = GaussianMixture(a=a, b=b)\n",
    "x_grid = np.linspace(-3, 7, 100)\n",
    "cdf_values = gm1.cdf(x_grid)\n",
    "print(\n",
    "    \"Plots are different time to time due to random weights generation.\",\n",
    "    f\"\\nCurrent weights: {gm1._weights.flatten()}\",\n",
    ")\n",
    "\n",
    "pdf_values = gm1.pdf(x_grid)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_grid, pdf_values, label=\"GMM PDF\", color=\"blue\")\n",
    "plt.title(\"PDF of Gaussian Mixture Model\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(gm1.sample(size=1000), kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "samples = gm1.sample(100)\n",
    "x = sorted(samples)\n",
    "plt.figure(figsize=(8, 5))\n",
    "origCDF = gm1.cdf(x)\n",
    "plt.plot(x, origCDF, label=\"Original CDF\", color=\"black\")\n",
    "empirCDF = EmpiricalCDF(samples)\n",
    "plt.plot(x, empirCDF, label=\"Empirical CDF\", color=\"green\")\n",
    "plt.title(\"CDF of Gaussian Mixture Model\")\n",
    "plt.xlabel(\"Random variable values\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Number of samples: {len(samples)}\",\n",
    "    f\"Mean absolute error: {mean_absolute_error(origCDF, empirCDF) * 100:.2f}%\",\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds\n",
    "\n",
    "\n",
    "def distance(params, x, dist=np.abs):\n",
    "    mu, sigma, weight = params.reshape(3, 3, -1)\n",
    "    # Evaluate KS statistic: max |F_n(t) - F(t; mu, sigma)|\n",
    "    # We only need to check at data points (and possibly midpoints); a common simple approach:\n",
    "    # F_n = np.arange(1, n+1) / n\n",
    "    F_n = EmpiricalCDF(x, x)\n",
    "    # Values t_i = x_sorted\n",
    "    gaus = stats.norm(loc=mu, scale=sigma)\n",
    "    F_theory = np.sum(weight * gaus.cdf(x), axis=0)\n",
    "    # Evaluate KS statistic: max |F_n(t) - F(t; mu, sigma)|\n",
    "    ks_vals = dist(F_n - F_theory)\n",
    "    # Also check left limits just before each x_i (optional for more accuracy)\n",
    "    # We skip for simplicity; KS is often evaluated at data points with this approach.\n",
    "    return float(np.max(ks_vals))\n",
    "\n",
    "\n",
    "def distF(x):\n",
    "    return lambda params: distance(params, x)\n",
    "\n",
    "\n",
    "initial_guess = [[0, 1, 2], [1, 1, 1], np.random.dirichlet(np.ones(3))]\n",
    "\n",
    "initial_guess = np.array(initial_guess).flatten()\n",
    "\n",
    "\n",
    "bounds = Bounds(\n",
    "    [-10, -10, -10, 1e-3, 1e-3, 1e-3, 0, 0, 0], [10, 10, 10, 10, 10, 10, 1, 1, 1]\n",
    ")\n",
    "\n",
    "method = \"nelder-mead\"\n",
    "x = gm1.sample(size=1000)\n",
    "res = minimize(distF(x), initial_guess, method=method, bounds=bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.x.reshape(3, -1), gm1.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def ecdf(x):\n",
    "    # returns a function F_hat(t) that evaluates the empirical CDF at t\n",
    "    x_sorted = np.sort(x)\n",
    "    n = x.size\n",
    "\n",
    "    def F_hat(t):\n",
    "        # proportion of observations <= t\n",
    "        return np.searchsorted(x_sorted, t, side=\"right\") / n\n",
    "\n",
    "    return F_hat\n",
    "\n",
    "\n",
    "def ks_distance(params, x):\n",
    "    mu, sigma = params\n",
    "    if sigma <= 0:\n",
    "        return np.inf\n",
    "    # empirical CDF\n",
    "    x_sorted = np.sort(x)\n",
    "    # Evaluate KS statistic: max |F_n(t) - F(t; mu, sigma)|\n",
    "    # We only need to check at data points (and possibly midpoints); a common simple approach:\n",
    "    # F_n = np.arange(1, n+1) / n\n",
    "    F_n = EmpiricalCDF(x_sorted, x_sorted)\n",
    "    # Values t_i = x_sorted\n",
    "    F_theory = norm.cdf(x_sorted, loc=mu, scale=sigma)\n",
    "    ks_vals = np.abs(F_n - F_theory)\n",
    "    # Also check left limits just before each x_i (optional for more accuracy)\n",
    "    # We skip for simplicity; KS is often evaluated at data points with this approach.\n",
    "    return float(np.max(ks_vals))\n",
    "\n",
    "\n",
    "def cvm_distance(params, x):\n",
    "    mu, sigma = params\n",
    "    if sigma <= 0:\n",
    "        return np.inf\n",
    "    x_sorted = np.sort(x)\n",
    "    n = x.size\n",
    "    # CvM distance approximation using the Watson/CvM formula:\n",
    "    # For ECDF F_n and theoretical F, one form is:\n",
    "    # CvM = (1/n) * sum_{i=1}^n [F_n(x_(i)) - F(x_(i); mu,sigma))]^2\n",
    "    # Here F_n(x_(i)) = i/n\n",
    "    F_theory = norm.cdf(x_sorted, loc=mu, scale=sigma)\n",
    "    CvM = np.mean((np.arange(1, n + 1) / n - F_theory) ** 2)\n",
    "    return float(CvM)\n",
    "\n",
    "\n",
    "def fit_normal_by_distance(x, distance_type=\"KS\", initial=None, bounds=None):\n",
    "    # initial guess for (mu, sigma)\n",
    "    if initial is None:\n",
    "        mu0 = np.mean(x)\n",
    "        sigma0 = np.std(x, ddof=1)\n",
    "        initial = np.array([mu0, max(sigma0, 1e-6)])\n",
    "    if bounds is None:\n",
    "        # sigma must be positive\n",
    "        bounds = [(None, None), (1e-9, None)]\n",
    "    if distance_type == \"KS\":\n",
    "\n",
    "        def dist_func(p):\n",
    "            return ks_distance(p, x)\n",
    "    elif distance_type == \"CvM\" or distance_type == \"CvM_distance\":\n",
    "\n",
    "        def dist_func(p):\n",
    "            return cvm_distance(p, x)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported distance_type. Choose 'KS' or 'CvM'.\")\n",
    "\n",
    "    method = \"nelder-mead\"\n",
    "    res = minimize(dist_func, initial, bounds=bounds, method=method)\n",
    "\n",
    "    mu_hat, sigma_hat = res.x\n",
    "    return {\n",
    "        \"mu_hat\": float(mu_hat),\n",
    "        \"sigma_hat\": float(sigma_hat),\n",
    "        \"distance\": float(res.fun),\n",
    "        \"success\": bool(res.success),\n",
    "        \"nit\": int(res.nit) if hasattr(res, \"nit\") else None,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # generate some sample data\n",
    "    seed = np.random.randint(0, 10000)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = 200\n",
    "    true_mu = 2.5\n",
    "    true_sigma = 1.2\n",
    "    x = rng.normal(loc=true_mu, scale=true_sigma, size=n)\n",
    "\n",
    "    # Fit by KS distance\n",
    "    res_ks = fit_normal_by_distance(x, distance_type=\"KS\")\n",
    "    print(\"KS fitting results:\")\n",
    "    print(res_ks)\n",
    "\n",
    "    # Fit by CvM distance\n",
    "    res_cvm = fit_normal_by_distance(x, distance_type=\"CvM\")\n",
    "    print(\"CvM fitting results:\")\n",
    "    print(res_cvm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finito-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
