{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### 1.2. Minimization problem on distribution\n",
    "\n",
    "Discrete approximation $F_{\\Delta S, T}(x)$ of the continuous distribution \n",
    "$F_{dS}(x)$ can be estimated in two ways: theoretical and empirical. \n",
    "\n",
    "On one hand:\n",
    "$$\n",
    "F^{(theor)}_{\\Delta S, T}(s) =\n",
    "\\sum_{k=1}^{K}{p_k \\cdot \\Phi \\left( \\frac{s - y_k \\Delta t}{z_k \\Delta t} \\right)} \n",
    "\\xrightarrow{K \\rightarrow \\infty , \\,\\Delta t \\rightarrow 0} F_{dS}\n",
    "$$\n",
    "\n",
    "But on the other we can estimate empirical distribution of $\\Delta S$ as:\n",
    "\n",
    "$$\n",
    "F^{(emp)}_{\\Delta S, T}(s) = \n",
    "\\frac{1}{T} \\cdot \\sum_{j=1}^{T} \\mathbb{I} \\left(\\Delta S < s\\right)\n",
    "\\xrightarrow{\\Delta t \\rightarrow 0} F_{dS}\n",
    "$$\n",
    "\n",
    "For evaluation of parameters $V_k = \\{y_k, z_k, p_k\\}$ we can set an \n",
    "minimization problem on distance (in some metric $\\rho(f,g)$) between\n",
    "theoretical and empirical distributions:\n",
    "\n",
    "\n",
    "$$ \n",
    "\\begin{equation}\n",
    "\\left\\{ \\begin{aligned} \n",
    "\n",
    "    \\min_{\\{V_k\\}_{k=1}^{K}} \\rho \\left(F^{(emp)}_{\\Delta S, T}, F^{(theor)}_{\\Delta S, T}\\right) \\\\\n",
    "    p_k \\geq 0 \\; \\wedge \\; \\sum_{k=1}^{K}p_k = 1 \\\\\n",
    "    0 \\leq z_k \\in B \\subset \\mathbb{R^+} \\\\\n",
    "    y_k \\in A \\subset \\mathbb{R}\n",
    "\n",
    "\n",
    "\n",
    "\\end{aligned} \\right.\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "We have a problem of approximation of known function - empirical distribution.\n",
    "\n",
    "<!-- Let's denote observed and calculated values as next:\n",
    "$$\n",
    "\\mathbf{x} = \\left[x_1, x_2, \\dots, x_N\\right] \\text{ - input values,} \\\\\n",
    "\\mathbf{a} = \\Delta t \\cdot \\left[y_1, y_2, \\dots, y_K\\right] \\text{- expectation values,}\\\\\n",
    "\\mathbf{b} = \\Delta t \\cdot \\left[z_1, z_2, \\dots, z_K\\right] \\text{- standard deviation values,}\\\\\n",
    "\\mathbf{p} = \\left[p_1, p_2, \\dots, p_K\\right] \\text{- joint probability density values}\\\\\n",
    "\\mathbf{y} = F^{(emp)}_{\\Delta S, T}(x) = \\bigl\\{F^{(emp)}_{\\Delta S, T} = f\\bigr\\} = \\left[f(x_1), f(x_2), \\dots, f(x_M)\\right] \\text{- targets} \\\\\n",
    "\\^{y} = p \\cdot \\Phi{\\left( \\right)} \\\\ \n",
    "$$ -->\n",
    "\n",
    "$$\n",
    "\\^{y} = p_1 \\cdot \\Phi{\\left( c_1\\right)} + \\dots + p_K \\cdot \\Phi{\\left( c_K\\right)} \\\\\n",
    "c_n = w_n \\cdot x + b_n\n",
    "$$\n",
    "\n",
    "<img src=\"../src/docs/static/nn_gaussian_mixture.svg\" align=\"center\" width=\"700\"/>\n",
    "\n",
    "\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3\\\\\n",
    "a & b & c\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "\n",
    "https://docs.pytorch.org/tutorials/intermediate/parametrizations.html\n",
    "https://medium.com/@minh.hoque/demystifying-neural-network-normalization-techniques-4a21d35b14f8\n",
    "https://towardsdatascience.com/deep-dive-into-softmax-regression-62deea103cb8/\n",
    "https://www.geeksforgeeks.org/deep-learning/what-is-layer-normalization/\n",
    "https://medium.com/@aya_hesham/function-approximation-with-deep-learning-a-practical-guide-with-code-examples-b817a59755af\n",
    "\n",
    "\n",
    "```{python}\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConstrainedLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.eps = eps\n",
    "        # Define raw weights as a learnable parameter\n",
    "        self.weight_raw = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize the raw weights to sum to 1 along a specific dimension\n",
    "        # The .clamp(min=self.eps) prevents division by zero if the sum is zero\n",
    "        normalized_weight = self.weight_raw / self.weight_raw.sum(dim=1, keepdim=True).clamp(min=self.eps)\n",
    "        return torch.nn.functional.linear(x, normalized_weight, self.bias)\n",
    "\n",
    "# Example usage\n",
    "model = ConstrainedLinear(in_features=10, out_features=5)\n",
    "input_tensor = torch.randn(1, 10)\n",
    "output = model(input_tensor)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "c = tf.Tensor([1.0])\n",
    "c * stats.norm.cdf(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "from finito.gauss import GaussianMixture\n",
    "\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "\n",
    "class SumToOne(tf.keras.constraints.Constraint):\n",
    "    def __call__(self, w):\n",
    "        # Calculate the sum of the absolute values of the weights\n",
    "        # Using absolute values ensures positivity before summing\n",
    "        sum_w = backend.sum(backend.abs(w))\n",
    "        # If the sum is zero, prevent division by zero and return original weights\n",
    "        # Otherwise, normalize the weights by dividing by their sum\n",
    "        return w / (sum_w + backend.epsilon())\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config()\n",
    "\n",
    "\n",
    "window_size = 100\n",
    "K = 3\n",
    "\n",
    "\n",
    "def norm_cdf(x):\n",
    "    # Î¦(x) = 0.5 * (1 + erf(x / sqrt(2)))\n",
    "    return 0.5 * (1.0 + tf.math.erf(x / tf.sqrt(2.0)))\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(1,)),\n",
    "        #   tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(K, activation=norm_cdf, use_bias=True),\n",
    "        # tf.keras.layers.Dense(K, kernel_constraint=SumToOne(), use_bias=False),\n",
    "        # tf.keras.layers.Dense(1, kernel_constraint=SumToOne(), use_bias=False),\n",
    "        tf.keras.layers.Dense(1),\n",
    "        #   tf.keras.layers.Dropout(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [-1, 3, 5]\n",
    "b = [1, 0.5, 0.2]\n",
    "p = [0.2, 0.5, 0.3]\n",
    "\n",
    "gm = GaussianMixture(a=a, b=b, p=p)\n",
    "samples = gm.sample(1000)\n",
    "x_train = np.sort(samples)\n",
    "# x_train = np.linspace(-3, 7, 100)\n",
    "y_train = gm.cdf(x_train)\n",
    "# px.line(x=x_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.fit(x_train, y_train, epochs=100)  # Train for a sufficient number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print info about weights\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.sort(gm.sample(1000))\n",
    "y_test = gm.cdf(x_test)\n",
    "x = np.expand_dims(x_test, axis=1)\n",
    "\n",
    "predictions = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"x\": x_test, \"true\": y_test, \"nn\": predictions.flatten()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df, x=x_test, y=[\"true\", \"nn\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finito-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
