{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### 1.2. Minimization problem on distribution\n",
    "\n",
    "Discrete approximation $F_{\\Delta S, T}(x)$ of the continuous distribution \n",
    "$F_{dS}(x)$ can be estimated in two ways: theoretical and empirical. \n",
    "\n",
    "On one hand:\n",
    "$$\n",
    "F^{(theor)}_{\\Delta S, T}(s) =\n",
    "\\sum_{k=1}^{K}{p_k \\cdot \\Phi \\left( \\frac{s - y_k \\Delta t}{z_k \\Delta t} \\right)} \n",
    "\\xrightarrow{K \\rightarrow \\infty , \\,\\Delta t \\rightarrow 0} F_{dS}\n",
    "$$\n",
    "\n",
    "But on the other we can estimate empirical distribution of $\\Delta S$ as:\n",
    "\n",
    "$$\n",
    "F^{(emp)}_{\\Delta S, T}(s) = \n",
    "\\frac{1}{T} \\cdot \\sum_{j=1}^{T} \\mathbb{I} \\left(\\Delta S < s\\right)\n",
    "\\xrightarrow{\\Delta t \\rightarrow 0} F_{dS}\n",
    "$$\n",
    "\n",
    "For evaluation of parameters $V_k = \\{y_k, z_k, p_k\\}$ we can set an \n",
    "minimization problem on distance (in some metric $\\rho(f,g)$) between\n",
    "theoretical and empirical distributions:\n",
    "\n",
    "\n",
    "$$ \n",
    "\\begin{equation}\n",
    "\\left\\{ \\begin{aligned} \n",
    "\n",
    "    \\min_{\\{V_k\\}_{k=1}^{K}} \\rho \\left(F^{(emp)}_{\\Delta S, T}, F^{(theor)}_{\\Delta S, T}\\right) \\\\\n",
    "    p_k \\geq 0 \\; \\wedge \\; \\sum_{k=1}^{K}p_k = 1 \\\\\n",
    "    0 \\leq z_k \\in B \\subset \\mathbb{R^+} \\\\\n",
    "    y_k \\in A \\subset \\mathbb{R}\n",
    "\n",
    "\n",
    "\n",
    "\\end{aligned} \\right.\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "We have a problem of approximation of known function - empirical distributon\n",
    "Let's denote observed and calculated values as next:\n",
    "$$\n",
    "\\mathbf{x} = \\left[x_1, x_2, \\dots, x_N\\right] \\text{ - input values,} \\\\\n",
    "\\mathbf{a} = \\Delta t \\cdot \\left[y_1, y_2, \\dots, y_K\\right] \\text{- expectation values,}\\\\\n",
    "\\mathbf{b} = \\Delta t \\cdot \\left[z_1, z_2, \\dots, z_K\\right] \\text{- standard deviation values,}\\\\\n",
    "\\mathbf{p} = \\left[p_1, p_2, \\dots, p_K\\right] \\text{- joint probability density values}\\\\\n",
    "\\mathbf{y} = F^{(emp)}_{\\Delta S, T}(x) = \\bigl\\{F^{(emp)}_{\\Delta S, T} = f\\bigr\\} = \\left[f(x_1), f(x_2), \\dots, f(x_M)\\right] \\text{- targets} \\\\\n",
    "$$\n",
    "\n",
    "Then we can write theoretical distribution in the next form:\n",
    "$$\n",
    "\\^{y} = p \\cdot \\Phi{\\left( \\right)} \\\\ \n",
    "\\^{y} = p_1 \\cdot \\Phi{\\left( c_1\\right)} + \\dots + p_K \\cdot \\Phi{\\left( c_K\\right)} \\\\\n",
    "c_n = \n",
    "$$\n",
    "\n",
    "<img src=\"../src/docs/nn.svg\" alt=\"nn\" class=\"bg-primary\" width=\"1400px\">\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3\\\\\n",
    "a & b & c\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "\n",
    "https://docs.pytorch.org/tutorials/intermediate/parametrizations.html\n",
    "https://medium.com/@minh.hoque/demystifying-neural-network-normalization-techniques-4a21d35b14f8\n",
    "https://towardsdatascience.com/deep-dive-into-softmax-regression-62deea103cb8/\n",
    "https://www.geeksforgeeks.org/deep-learning/what-is-layer-normalization/\n",
    "https://medium.com/@aya_hesham/function-approximation-with-deep-learning-a-practical-guide-with-code-examples-b817a59755af\n",
    "\n",
    "\n",
    "```{python}\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConstrainedLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.eps = eps\n",
    "        # Define raw weights as a learnable parameter\n",
    "        self.weight_raw = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize the raw weights to sum to 1 along a specific dimension\n",
    "        # The .clamp(min=self.eps) prevents division by zero if the sum is zero\n",
    "        normalized_weight = self.weight_raw / self.weight_raw.sum(dim=1, keepdim=True).clamp(min=self.eps)\n",
    "        return torch.nn.functional.linear(x, normalized_weight, self.bias)\n",
    "\n",
    "# Example usage\n",
    "model = ConstrainedLinear(in_features=10, out_features=5)\n",
    "input_tensor = torch.randn(1, 10)\n",
    "output = model(input_tensor)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "window_size = 100\n",
    "K = 64\n",
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(1,)),\n",
    "        #   tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(K, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(2 * K, activation=\"relu\"),\n",
    "        #   tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.linspace(0, 2 * np.pi, 100)\n",
    "y_train = np.sin(x_train)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.fit(x_train, y_train, epochs=1000)  # Train for a sufficient number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(0, 2 * np.pi, 250)\n",
    "y_test = np.sin(x_test)\n",
    "x = np.expand_dims(x_test, axis=1)\n",
    "\n",
    "predictions = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"x\": x_test, \"sin\": y_test, \"nn\": predictions.flatten()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.line(df, y=[\"sin\", \"nn\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finito-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
