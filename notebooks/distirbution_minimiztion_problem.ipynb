{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Minimization problem on distribution\n",
    "\n",
    "Discrete approximation $F_{\\Delta S, T}(x)$ of the continuous distribution \n",
    "$F_{dS}(x)$ can be estimated in two ways: theoretical and empirical. \n",
    "\n",
    "On one hand:\n",
    "$$\n",
    "F^{(theor)}_{\\Delta S, T}(s) =\n",
    "\\sum_{k=1}^{K}{p_k \\cdot \\Phi \\left( \\frac{s - y_k \\Delta t}{z_k \\Delta t} \\right)} \n",
    "\\xrightarrow{K \\rightarrow \\infty , \\,\\Delta t \\rightarrow 0} F_{dS}\n",
    "$$\n",
    "\n",
    "But on the other we can estimate empirical distribution of $\\Delta S$ as:\n",
    "\n",
    "$$\n",
    "F^{(emp)}_{\\Delta S, T}(s) = \n",
    "\\frac{1}{T} \\cdot \\sum_{j=1}^{T} \\mathbb{I} \\left(\\Delta S < s\\right)\n",
    "\\xrightarrow{\\Delta t \\rightarrow 0} F_{dS}\n",
    "$$\n",
    "\n",
    "For evaluation of parameters $V_k = \\{y_k, z_k, p_k\\}$ we can set an \n",
    "minimization problem on distance (in some metric $\\rho(f,g)$) between\n",
    "theoretical and empirical distributions:\n",
    "\n",
    "\n",
    "$$ \n",
    "\\begin{equation}\n",
    "\\left\\{ \\begin{aligned} \n",
    "\n",
    "    \\min_{\\{V_k\\}_{k=1}^{K}} \\rho \\left(F^{(emp)}_{\\Delta S, T}, F^{(theor)}_{\\Delta S, T}\\right) \\\\\n",
    "    p_k \\geq 0 \\; \\wedge \\; \\sum_{k=1}^{K}p_k = 1 \\\\\n",
    "    0 \\leq z_k \\in B \\subset \\mathbb{R^+} \\\\\n",
    "    y_k \\in A \\subset \\mathbb{R}\n",
    "\n",
    "\n",
    "\n",
    "\\end{aligned} \\right.\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "Our goal is to estimate parameters of the non-linear function \n",
    "$\n",
    "g(s) =\n",
    "\\sum_{k=1}^{K}{p_k \\cdot \\Phi{\\left( \\frac{s - y_k \\Delta t}{z_k \\Delta t} \\right)}}\n",
    "$\n",
    "if we know the target value $f(s)=F^{(emp)}(s)$ in that point.\n",
    "\n",
    "By the view of the function $g$ we can see, that input $s$ is initially \n",
    "undergo linear transformation - \n",
    "$$\n",
    "\\frac{s - y_k \\Delta t}{z_k \\Delta t} =\n",
    "\\frac{s}{z_k \\Delta t} - \\frac{y_k \\Delta t}{z_k \\Delta t} =\n",
    "w_k \\cdot s + b_k = c_k\\quad\n",
    "\\text{, where }\n",
    "w_k = \\frac{1}{z_k \\Delta t},\\;\n",
    "b_k = - \\frac{y_k}{z_k}\n",
    "$$\n",
    "\n",
    "Next non-linear function $\\Phi$ - cumulative distribution function of the normal\n",
    "law - is applied. \n",
    "And after that once again linear transformation is considered.\n",
    "\n",
    "As we can see it yields next model:\n",
    "\n",
    "<!-- Let's denote observed and calculated values as next:\n",
    "$$\n",
    "\\mathbf{x} = \\left[x_1, x_2, \\dots, x_N\\right] \\text{ - input values,} \\\\\n",
    "\\mathbf{a} = \\Delta t \\cdot \\left[y_1, y_2, \\dots, y_K\\right] \\text{- expectation values,}\\\\\n",
    "\\mathbf{b} = \\Delta t \\cdot \\left[z_1, z_2, \\dots, z_K\\right] \\text{- standard deviation values,}\\\\\n",
    "\\mathbf{p} = \\left[p_1, p_2, \\dots, p_K\\right] \\text{- joint probability density values}\\\\\n",
    "\\mathbf{y} = F^{(emp)}_{\\Delta S, T}(x) = \\bigl\\{F^{(emp)}_{\\Delta S, T} = f\\bigr\\} = \\left[f(x_1), f(x_2), \\dots, f(x_M)\\right] \\text{- targets} \\\\\n",
    "\\^{y} = p \\cdot \\Phi{\\left( \\right)} \\\\ \n",
    "$$ -->\n",
    "<center>\n",
    "    <img src=\"../src/docs/static/nn_gaussian_mixture.svg\" width=\"700\"/>\n",
    "</center>\n",
    "\n",
    "It's a neural network with two linear layers (blue) and one activation (green)\n",
    "(a.k.a multilayer perceptron), that can be written as:\n",
    "$$\n",
    "g(x) = \\Phi{\\left(x \\cdot \\mathbf{w} + \\mathbf{b}\\right)} \\times \\mathbf{p}^T, \\;\n",
    "g:\\mathbb{R} \\rightarrow \\mathbb{R}\n",
    "\\text{, where}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{b} = \n",
    "\\begin{bmatrix}\n",
    "b_1\\\\\n",
    "b_2\\\\\n",
    "\\vdots\\\\\n",
    "b_K\n",
    "\\end{bmatrix},\n",
    "\n",
    "\\mathbf{w} = \n",
    "\\begin{bmatrix}\n",
    "w_1^{(1)}\\\\\n",
    "w_2^{(1)}\\\\\n",
    "\\vdots\\\\\n",
    "w_K^{(1)}\n",
    "\\end{bmatrix},\n",
    "\n",
    "\\mathbf{p} = \n",
    "\\begin{bmatrix}\n",
    "w_1^{(2)}\\\\\n",
    "w_2^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "w_K^{(2)}\n",
    "\\end{bmatrix}\n",
    "\\text{, and} \\quad\n",
    "\\forall i\\in\\{1,2,\\dots,K\\}: \\quad w_i^{(1)}, w_i^{(2)} > 0 \\;\\wedge \\mathbf{p} \\cdot \\overrightarrow{1} = 1\n",
    "$$\n",
    "\n",
    "And the goal is to minimize some arbitrary loss-function $L(g, s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "from finito.gauss import GaussianMixture\n",
    "\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "\n",
    "class SumToOne(tf.keras.constraints.Constraint):\n",
    "    def __call__(self, w):\n",
    "        # Calculate the sum of the absolute values of the weights\n",
    "        # Using absolute values ensures positivity before summing\n",
    "        sum_w = backend.sum(backend.abs(w))\n",
    "        # If the sum is zero, prevent division by zero and return original weights\n",
    "        # Otherwise, normalize the weights by dividing by their sum\n",
    "        return w / (sum_w + backend.epsilon())\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config()\n",
    "\n",
    "\n",
    "window_size = 100\n",
    "K = 3\n",
    "\n",
    "\n",
    "def norm_cdf(x):\n",
    "    # Î¦(x) = 0.5 * (1 + erf(x / sqrt(2)))\n",
    "    return 0.5 * (1.0 + tf.math.erf(x / tf.sqrt(2.0)))\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(1,)),\n",
    "        #   tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(K, activation=norm_cdf, use_bias=True),\n",
    "        # tf.keras.layers.Dense(K, kernel_constraint=SumToOne(), use_bias=False),\n",
    "        # tf.keras.layers.Dense(1, kernel_constraint=SumToOne(), use_bias=False),\n",
    "        tf.keras.layers.Dense(1),\n",
    "        #   tf.keras.layers.Dropout(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [-1, 3, 5]\n",
    "b = [1, 0.5, 0.2]\n",
    "p = [0.2, 0.5, 0.3]\n",
    "\n",
    "gm = GaussianMixture(a=a, b=b, p=p)\n",
    "samples = gm.sample(1000)\n",
    "x_train = np.sort(samples)\n",
    "# x_train = np.linspace(-3, 7, 100)\n",
    "y_train = gm.cdf(x_train)\n",
    "# px.line(x=x_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.fit(x_train, y_train, epochs=100)  # Train for a sufficient number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print info about weights\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.sort(gm.sample(1000))\n",
    "y_test = gm.cdf(x_test)\n",
    "x = np.expand_dims(x_test, axis=1)\n",
    "\n",
    "predictions = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"x\": x_test, \"true\": y_test, \"nn\": predictions.flatten()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df, x=x_test, y=[\"true\", \"nn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "- [link 1](https://docs.pytorch.org/tutorials/intermediate/parametrizations.html)\n",
    "\n",
    "- [link 2](https://medium.com/@minh.hoque/demystifying-neural-network-normalization-techniques-4a21d35b14f8)\n",
    "- [link 3](https://towardsdatascience.com/deep-dive-into-softmax-regression-62deea103cb8/)\n",
    "- [link 4](https://www.geeksforgeeks.org/deep-learning/what-is-layer-normalization/)\n",
    "- [link 5](https://medium.com/@aya_hesham/function-approximation-with-deep-learning-a-practical-guide-with-code-examples-b817a59755af)\n",
    "\n",
    "\n",
    "```{python}\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConstrainedLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.eps = eps\n",
    "        # Define raw weights as a learnable parameter\n",
    "        self.weight_raw = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize the raw weights to sum to 1 along a specific dimension\n",
    "        # The .clamp(min=self.eps) prevents division by zero if the sum is zero\n",
    "        normalized_weight = self.weight_raw / self.weight_raw.sum(dim=1, keepdim=True).clamp(min=self.eps)\n",
    "        return torch.nn.functional.linear(x, normalized_weight, self.bias)\n",
    "\n",
    "# Example usage\n",
    "model = ConstrainedLinear(in_features=10, out_features=5)\n",
    "input_tensor = torch.randn(1, 10)\n",
    "output = model(input_tensor)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finito-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
